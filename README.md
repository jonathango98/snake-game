# Snake AI

This project explores two different approaches to building an AI to play the classic game of Snake:

1.  **Neural Network Approach:** A supervised learning approach where a neural network is trained on data generated from a human playing the game.
2.  **Reinforcement Learning Approach:** An unsupervised learning approach using Deep Q-Networks (DQN) where the agent learns to play the game through trial and error.

## Installation

To run this project, you need to have Python 3 installed. You can install the required dependencies using pip:

```bash
pip install -r requirements.txt
```

## Usage

### Neural Network

To train the neural network, you first need to generate training data by playing the game yourself.

1.  **Generate Data:**
    ```bash
    python neural_network/snake.py
    ```
    Play the game, and the data will be saved to a `.csv` file in the `neural_network/data` directory.

2.  **Train the Model:**
    ```bash
    python neural_network/train.py
    ```
    This will train the neural network on the generated data and save the model.
    The `--model` argument specifies the type of model to train. Choices include: `knn`, `svm`, `tf`, `pytorch`.

3.  **Run the AI:**
    ```bash
    python neural_network/run.py --file <path_to_model>
    ```
    This will run the game with the trained neural network playing.
    The `--file` argument specifies the path to the trained model file (e.g., `.pth`, `.h5`, or `.pkl`).

#### Model Input

The neural network model receives a 17-element feature vector as input, representing the game's state relative to the snake's head. These features are generated by the `get_state` method in `neural_network/snake.py` and include:

*   **Distance Sensors (3):** Normalized distances to obstacles (wall, self) in three directions (e.g., straight, left, right).
*   **Immediate Danger (3):** Binary flags indicating immediate collision danger in three directions.
*   **Absolute Direction (4):** A one-hot encoded vector representing the snake's current absolute direction (Up, Down, Left, Right).
*   **Food Position (4):** Binary flags indicating the relative position of the food (e.g., food is up, food is down, food is left, food is right).
*   **Distance to Tail (1):** Normalized distance to the snake's own tail.
*   **Body Distribution (2):** Floats representing the ratio of the snake's body on its left versus its right side.

This feature vector allows the model to make informed decisions based on its immediate surroundings and strategic information.

### Reinforcement Learning

To train the reinforcement learning agent:

```bash
python reinforcement_learning/dqn.py
```

This will train the DQN agent and save the model. You can see the agent learning in real-time.
